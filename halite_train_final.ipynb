{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Halite 4 Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Halite 4 was a competitive simulation challenge run by Kaggle in 2020. It is a 4 player resource management game where teams create agents to compete against other teams' agents. https://www.kaggle.com/c/halite/\n",
    "\n",
    "This notebook is part of the solution from team \"KhaVo Dan Gilles Robga Tung\". We placed a provisional 8th at the time of the end of the public phase and hope to remain in the top 10 once the provisional 7 day phase completes. You can read about our solution at https://www.kaggle.com/c/halite/discussion/183312\n",
    "\n",
    "Our solution includes code for:\n",
    "- A Machine Language driven agent that uses semantic segmentation of game boards to predict best next actions, based on imitating thousands of previous competitor games.\n",
    "- Pretrained pytorch weights for the ML agent's model.\n",
    "- A notebook to create a numpy dataset from episode.json's\n",
    "- A notebook to train an agent based on the dataset\n",
    "\n",
    "Training will take 5-10 hours on a consumer GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rules of Kaggle's halite challenge specified that no internet access was possible from the servers running the agent code. This meant that there was ability to install external libraries that were not in the kaggle base image. Instead, we need to include and reference any library code required. This does look ugly in a notebook. For our solution we use:\n",
    "- EfficientNet by Luke Melas https://github.com/lukemelas/EfficientNet-PyTorch\n",
    "- Unet by Qubvel https://github.com/qubvel/segmentation_models.pytorch\n",
    "- We are indebted to the input and output stack ideas from David NQ https://www.kaggle.com/david1013/pytorch-starter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data location config\n",
    "TRAIN_DIR = \"/tmp/halite/train/\"\n",
    "LOG_DIR = \"/tmp/halite/logs/\"\n",
    "\n",
    "# GPU config\n",
    "CUDA_VISIBLE_DEVICES = \"0\"  # \"\" - CPU, \"0\" - 1 GPU, \"0,1\" - MultiGPU\n",
    "is_fp16_used = True\n",
    "BATCH_SIZE = 560 # suitable for a RTX 2080 Ti\n",
    "device = 'cuda:0'\n",
    "\n",
    "# Model config\n",
    "LR = 0.1 \n",
    "ENCODER_LR = 0.05\n",
    "NUMBER_OF_INPUT_LAYERS = 29\n",
    "CLASSES = 5 # STILL+NSEW, \n",
    "ENCODER_DEPTH = 2 # unet\n",
    "DECODER_CHANNELS =(64, 32) # unet\n",
    "\n",
    "\n",
    "# training config\n",
    "NUM_EPOCHS = 30             # convergence usually reached epoch 15-20 \n",
    "EPOCH_LENGTH = 1500000      # Restrict epochs to this length or len(images) if smaller. 1 match == 400\n",
    "SCHEDULER_PATIENCE = 2      # be patient for this many epochs before stepping LR\n",
    "SCHEDULER_FACTOR = 0.4      # reduce LR by this factor after expired patience\n",
    "EARLY_STOPPING_PATIENCE = 6 # stop training after this many epochs without improvement\n",
    "SAVE_N_BEST = 9             # number of best checkpoints to save\n",
    "USE_DIHEDRAL_AUG = True     # flips and rotates for 8x inputs\n",
    "SEED = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Only tested for torch: 1.6.0, catalyst: 20.07 - untested before / beyond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from types import SimpleNamespace  \n",
    "import random\n",
    "import functools\n",
    "from functools import partial\n",
    "from typing import Optional, Union, List\n",
    "import re\n",
    "import math\n",
    "import collections\n",
    "from typing import List\n",
    "from collections import OrderedDict\n",
    "import datetime as datetime\n",
    "from typing import Callable, List, Tuple \n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Softmax\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import model_zoo\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch import optim\n",
    "from torchvision import datasets\n",
    "\n",
    "import catalyst\n",
    "from catalyst.contrib.nn import RAdam, Lookahead, DiceLoss, IoULoss, BCEDiceLoss\n",
    "from catalyst.dl import utils\n",
    "from catalyst.dl.callbacks import CriterionCallback, MetricAggregationCallback, CheckpointCallback, EarlyStoppingCallback\n",
    "from catalyst.dl.callbacks import DiceCallback, IouCallback, CriterionCallback, MetricAggregationCallback\n",
    "from catalyst.utils import unpack_checkpoint, load_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library code copies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#from efficientnet_pytorch import EfficientNet\n",
    "#from efficientnet_pytorch.utils import url_map, url_map_advprop, get_model_params\n",
    "\"\"\"utils.py - Helper functions for building the model and for loading model parameters.\n",
    "   These helper functions are built to mirror those in the official TensorFlow implementation.\n",
    "\"\"\"\n",
    "\n",
    "# Author: lukemelas (github username)\n",
    "# Github repo: https://github.com/lukemelas/EfficientNet-PyTorch\n",
    "# With adjustments and added comments by workingcoder (github username).\n",
    "\n",
    "\n",
    "################################################################################\n",
    "### Help functions for model architecture\n",
    "################################################################################\n",
    "\n",
    "# GlobalParams and BlockArgs: Two namedtuples\n",
    "# Swish and MemoryEfficientSwish: Two implementations of the method\n",
    "# round_filters and round_repeats:\n",
    "#     Functions to calculate params for scaling model width and depth ! ! !\n",
    "# get_width_and_height_from_size and calculate_output_image_size\n",
    "# drop_connect: A structural design\n",
    "# get_same_padding_conv2d:\n",
    "#     Conv2dDynamicSamePadding\n",
    "#     Conv2dStaticSamePadding\n",
    "# get_same_padding_maxPool2d:\n",
    "#     MaxPool2dDynamicSamePadding\n",
    "#     MaxPool2dStaticSamePadding\n",
    "#     It's an additional function, not used in EfficientNet,\n",
    "#     but can be used in other model (such as EfficientDet).\n",
    "# Identity: An implementation of identical mapping\n",
    "\n",
    "# Parameters for the entire model (stem, all blocks, and head)\n",
    "GlobalParams = collections.namedtuple('GlobalParams', [\n",
    "    'width_coefficient', 'depth_coefficient', 'image_size', 'dropout_rate',\n",
    "    'num_classes', 'batch_norm_momentum', 'batch_norm_epsilon',\n",
    "    'drop_connect_rate', 'depth_divisor', 'min_depth'])\n",
    "\n",
    "# Parameters for an individual model block\n",
    "BlockArgs = collections.namedtuple('BlockArgs', [\n",
    "    'num_repeat', 'kernel_size', 'stride', 'expand_ratio',\n",
    "    'input_filters', 'output_filters', 'se_ratio', 'id_skip'])\n",
    "\n",
    "# Set GlobalParams and BlockArgs's defaults\n",
    "GlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\n",
    "BlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\n",
    "\n",
    "\n",
    "# An ordinary implementation of Swish function\n",
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "# A memory-efficient implementation of Swish function\n",
    "class SwishImplementation(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i * torch.sigmoid(i)\n",
    "        ctx.save_for_backward(i)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        i = ctx.saved_tensors[0]\n",
    "        sigmoid_i = torch.sigmoid(i)\n",
    "        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n",
    "\n",
    "class MemoryEfficientSwish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return SwishImplementation.apply(x)\n",
    "\n",
    "\n",
    "def round_filters(filters, global_params):\n",
    "    \"\"\"Calculate and round number of filters based on width multiplier.\n",
    "       Use width_coefficient, depth_divisor and min_depth of global_params.\n",
    "    Args:\n",
    "        filters (int): Filters number to be calculated.\n",
    "        global_params (namedtuple): Global params of the model.\n",
    "    Returns:\n",
    "        new_filters: New filters number after calculating.\n",
    "    \"\"\"\n",
    "    multiplier = global_params.width_coefficient\n",
    "    if not multiplier:\n",
    "        return filters\n",
    "    # TODO: modify the params names.\n",
    "    #       maybe the names (width_divisor,min_width)\n",
    "    #       are more suitable than (depth_divisor,min_depth).\n",
    "    divisor = global_params.depth_divisor\n",
    "    min_depth = global_params.min_depth\n",
    "    filters *= multiplier\n",
    "    min_depth = min_depth or divisor # pay attention to this line when using min_depth\n",
    "    # follow the formula transferred from official TensorFlow implementation\n",
    "    new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)\n",
    "    if new_filters < 0.9 * filters: # prevent rounding by more than 10%\n",
    "        new_filters += divisor\n",
    "    return int(new_filters)\n",
    "\n",
    "\n",
    "def round_repeats(repeats, global_params):\n",
    "    \"\"\"Calculate module's repeat number of a block based on depth multiplier.\n",
    "       Use depth_coefficient of global_params.\n",
    "    Args:\n",
    "        repeats (int): num_repeat to be calculated.\n",
    "        global_params (namedtuple): Global params of the model.\n",
    "    Returns:\n",
    "        new repeat: New repeat number after calculating.\n",
    "    \"\"\"\n",
    "    multiplier = global_params.depth_coefficient\n",
    "    if not multiplier:\n",
    "        return repeats\n",
    "    # follow the formula transferred from official TensorFlow implementation\n",
    "    return int(math.ceil(multiplier * repeats))\n",
    "\n",
    "\n",
    "def drop_connect(inputs, p, training):\n",
    "    \"\"\"Drop connect.\n",
    "       \n",
    "    Args:\n",
    "        input (tensor: BCWH): Input of this structure.\n",
    "        p (float: 0.0~1.0): Probability of drop connection.\n",
    "        training (bool): The running mode.\n",
    "    Returns:\n",
    "        output: Output after drop connection.\n",
    "    \"\"\"\n",
    "    assert p >= 0 and p <= 1, 'p must be in range of [0,1]'\n",
    "\n",
    "    if not training:\n",
    "        return inputs\n",
    "\n",
    "    batch_size = inputs.shape[0]\n",
    "    keep_prob = 1 - p\n",
    "\n",
    "    # generate binary_tensor mask according to probability (p for 0, 1-p for 1)\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype, device=inputs.device)\n",
    "    binary_tensor = torch.floor(random_tensor)\n",
    "\n",
    "    output = inputs / keep_prob * binary_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_width_and_height_from_size(x):\n",
    "    \"\"\"Obtain height and width from x.\n",
    "    Args:\n",
    "        x (int, tuple or list): Data size.\n",
    "    Returns:\n",
    "        size: A tuple or list (H,W).\n",
    "    \"\"\"\n",
    "    if isinstance(x, int):\n",
    "        return x, x\n",
    "    if isinstance(x, list) or isinstance(x, tuple):\n",
    "        return x\n",
    "    else:\n",
    "        raise TypeError()\n",
    "\n",
    "\n",
    "def calculate_output_image_size(input_image_size, stride):\n",
    "    \"\"\"Calculates the output image size when using Conv2dSamePadding with a stride.\n",
    "       Necessary for static padding. Thanks to mannatsingh for pointing this out.\n",
    "    Args:\n",
    "        input_image_size (int, tuple or list): Size of input image.\n",
    "        stride (int, tuple or list): Conv2d operation's stride.\n",
    "    Returns:\n",
    "        output_image_size: A list [H,W].\n",
    "    \"\"\"\n",
    "    if input_image_size is None:\n",
    "        return None\n",
    "    image_height, image_width = get_width_and_height_from_size(input_image_size)\n",
    "    stride = stride if isinstance(stride, int) else stride[0]\n",
    "    image_height = int(math.ceil(image_height / stride))\n",
    "    image_width = int(math.ceil(image_width / stride))\n",
    "    return [image_height, image_width]\n",
    "\n",
    "\n",
    "# Note: \n",
    "# The following 'SamePadding' functions make output size equal ceil(input size/stride).\n",
    "# Only when stride equals 1, can the output size be the same as input size.\n",
    "# Don't be confused by their function names ! ! !\n",
    "\n",
    "def get_same_padding_conv2d(image_size=None):\n",
    "    \"\"\"Chooses static padding if you have specified an image size, and dynamic padding otherwise.\n",
    "       Static padding is necessary for ONNX exporting of models.\n",
    "    Args:\n",
    "        image_size (int or tuple): Size of the image.\n",
    "    Returns:\n",
    "        Conv2dDynamicSamePadding or Conv2dStaticSamePadding.\n",
    "    \"\"\"\n",
    "    if image_size is None:\n",
    "        return Conv2dDynamicSamePadding\n",
    "    else:\n",
    "        return partial(Conv2dStaticSamePadding, image_size=image_size)\n",
    "\n",
    "\n",
    "class Conv2dDynamicSamePadding(nn.Conv2d):\n",
    "    \"\"\"2D Convolutions like TensorFlow, for a dynamic image size.\n",
    "       The padding is operated in forward function by calculating dynamically.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tips for 'SAME' mode padding.\n",
    "    #     Given the following:\n",
    "    #         i: width or height\n",
    "    #         s: stride\n",
    "    #         k: kernel size\n",
    "    #         d: dilation\n",
    "    #         p: padding\n",
    "    #     Output after Conv2d:\n",
    "    #         o = floor((i+p-((k-1)*d+1))/s+1)\n",
    "    # If o equals i, i = floor((i+p-((k-1)*d+1))/s+1),\n",
    "    # => p = (i-1)*s+((k-1)*d+1)-i\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias)\n",
    "        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]] * 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        ih, iw = x.size()[-2:]\n",
    "        kh, kw = self.weight.size()[-2:]\n",
    "        sh, sw = self.stride\n",
    "        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw) # change the output size according to stride ! ! !\n",
    "        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
    "        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            x = F.pad(x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2])\n",
    "        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "\n",
    "class Conv2dStaticSamePadding(nn.Conv2d):\n",
    "    \"\"\"2D Convolutions like TensorFlow's 'SAME' mode, with the given input image size.\n",
    "       The padding mudule is calculated in construction function, then used in forward.\n",
    "    \"\"\"\n",
    "\n",
    "    # With the same calculation as Conv2dDynamicSamePadding\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, image_size=None, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride, **kwargs)\n",
    "        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]] * 2\n",
    "\n",
    "        # Calculate padding based on image size and save it\n",
    "        assert image_size is not None\n",
    "        ih, iw = (image_size, image_size) if isinstance(image_size, int) else image_size\n",
    "        kh, kw = self.weight.size()[-2:]\n",
    "        sh, sw = self.stride\n",
    "        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n",
    "        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
    "        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            self.static_padding = nn.ZeroPad2d((pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2))\n",
    "        else:\n",
    "            self.static_padding = Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.static_padding(x)\n",
    "        x = F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_same_padding_maxPool2d(image_size=None):\n",
    "    \"\"\"Chooses static padding if you have specified an image size, and dynamic padding otherwise.\n",
    "       Static padding is necessary for ONNX exporting of models.\n",
    "    Args:\n",
    "        image_size (int or tuple): Size of the image.\n",
    "    Returns:\n",
    "        MaxPool2dDynamicSamePadding or MaxPool2dStaticSamePadding.\n",
    "    \"\"\"\n",
    "    if image_size is None:\n",
    "        return MaxPool2dDynamicSamePadding\n",
    "    else:\n",
    "        return partial(MaxPool2dStaticSamePadding, image_size=image_size)\n",
    "\n",
    "\n",
    "class MaxPool2dDynamicSamePadding(nn.MaxPool2d):\n",
    "    \"\"\"2D MaxPooling like TensorFlow's 'SAME' mode, with a dynamic image size.\n",
    "       The padding is operated in forward function by calculating dynamically.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size, stride, padding=0, dilation=1, return_indices=False, ceil_mode=False):\n",
    "        super().__init__(kernel_size, stride, padding, dilation, return_indices, ceil_mode)\n",
    "        self.stride = [self.stride] * 2 if isinstance(self.stride, int) else self.stride\n",
    "        self.kernel_size = [self.kernel_size] * 2 if isinstance(self.kernel_size, int) else self.kernel_size\n",
    "        self.dilation = [self.dilation] * 2 if isinstance(self.dilation, int) else self.dilation\n",
    "\n",
    "    def forward(self, x):\n",
    "        ih, iw = x.size()[-2:]\n",
    "        kh, kw = self.kernel_size\n",
    "        sh, sw = self.stride\n",
    "        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n",
    "        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
    "        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            x = F.pad(x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2])\n",
    "        return F.max_pool2d(x, self.kernel_size, self.stride, self.padding,\n",
    "                            self.dilation, self.ceil_mode, self.return_indices)\n",
    "\n",
    "class MaxPool2dStaticSamePadding(nn.MaxPool2d):\n",
    "    \"\"\"2D MaxPooling like TensorFlow's 'SAME' mode, with the given input image size.\n",
    "       The padding mudule is calculated in construction function, then used in forward.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size, stride, image_size=None, **kwargs):\n",
    "        super().__init__(kernel_size, stride, **kwargs)\n",
    "        self.stride = [self.stride] * 2 if isinstance(self.stride, int) else self.stride\n",
    "        self.kernel_size = [self.kernel_size] * 2 if isinstance(self.kernel_size, int) else self.kernel_size\n",
    "        self.dilation = [self.dilation] * 2 if isinstance(self.dilation, int) else self.dilation\n",
    "\n",
    "        # Calculate padding based on image size and save it\n",
    "        assert image_size is not None\n",
    "        ih, iw = (image_size, image_size) if isinstance(image_size, int) else image_size\n",
    "        kh, kw = self.kernel_size\n",
    "        sh, sw = self.stride\n",
    "        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n",
    "        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
    "        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            self.static_padding = nn.ZeroPad2d((pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2))\n",
    "        else:\n",
    "            self.static_padding = Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.static_padding(x)\n",
    "        x = F.max_pool2d(x, self.kernel_size, self.stride, self.padding,\n",
    "                         self.dilation, self.ceil_mode, self.return_indices)\n",
    "        return x\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    \"\"\"Identity mapping.\n",
    "       Send input to output directly.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input\n",
    "\n",
    "\n",
    "################################################################################\n",
    "### Helper functions for loading model params\n",
    "################################################################################\n",
    "\n",
    "# BlockDecoder: A Class for encoding and decoding BlockArgs\n",
    "# efficientnet_params: A function to query compound coefficient\n",
    "# get_model_params and efficientnet:\n",
    "#     Functions to get BlockArgs and GlobalParams for efficientnet\n",
    "# url_map and url_map_advprop: Dicts of url_map for pretrained weights\n",
    "# load_pretrained_weights: A function to load pretrained weights\n",
    "\n",
    "class BlockDecoder(object):\n",
    "    \"\"\"Block Decoder for readability,\n",
    "       straight from the official TensorFlow repository.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _decode_block_string(block_string):\n",
    "        \"\"\"Get a block through a string notation of arguments.\n",
    "        Args:\n",
    "            block_string (str): A string notation of arguments.\n",
    "                                Examples: 'r1_k3_s11_e1_i32_o16_se0.25_noskip'.\n",
    "        Returns:\n",
    "            BlockArgs: The namedtuple defined at the top of this file.\n",
    "        \"\"\"\n",
    "        assert isinstance(block_string, str)\n",
    "\n",
    "        ops = block_string.split('_')\n",
    "        options = {}\n",
    "        for op in ops:\n",
    "            splits = re.split(r'(\\d.*)', op)\n",
    "            if len(splits) >= 2:\n",
    "                key, value = splits[:2]\n",
    "                options[key] = value\n",
    "\n",
    "        # Check stride\n",
    "        assert (('s' in options and len(options['s']) == 1) or\n",
    "                (len(options['s']) == 2 and options['s'][0] == options['s'][1]))\n",
    "\n",
    "        return BlockArgs(\n",
    "            num_repeat=int(options['r']),\n",
    "            kernel_size=int(options['k']),\n",
    "            stride=[int(options['s'][0])],\n",
    "            expand_ratio=int(options['e']),\n",
    "            input_filters=int(options['i']),\n",
    "            output_filters=int(options['o']),\n",
    "            se_ratio=float(options['se']) if 'se' in options else None,\n",
    "            id_skip=('noskip' not in block_string))\n",
    "\n",
    "    @staticmethod\n",
    "    def _encode_block_string(block):\n",
    "        \"\"\"Encode a block to a string.\n",
    "        Args:\n",
    "            block (namedtuple): A BlockArgs type argument.\n",
    "        Returns:\n",
    "            block_string: A String form of BlockArgs.\n",
    "        \"\"\"\n",
    "        args = [\n",
    "            'r%d' % block.num_repeat,\n",
    "            'k%d' % block.kernel_size,\n",
    "            's%d%d' % (block.strides[0], block.strides[1]),\n",
    "            'e%s' % block.expand_ratio,\n",
    "            'i%d' % block.input_filters,\n",
    "            'o%d' % block.output_filters\n",
    "        ]\n",
    "        if 0 < block.se_ratio <= 1:\n",
    "            args.append('se%s' % block.se_ratio)\n",
    "        if block.id_skip is False:\n",
    "            args.append('noskip')\n",
    "        return '_'.join(args)\n",
    "\n",
    "    @staticmethod\n",
    "    def decode(string_list):\n",
    "        \"\"\"Decode a list of string notations to specify blocks inside the network.\n",
    "        Args:\n",
    "            string_list (list[str]): A list of strings, each string is a notation of block.\n",
    "        Returns:\n",
    "            blocks_args: A list of BlockArgs namedtuples of block args.\n",
    "        \"\"\"\n",
    "        assert isinstance(string_list, list)\n",
    "        blocks_args = []\n",
    "        for block_string in string_list:\n",
    "            blocks_args.append(BlockDecoder._decode_block_string(block_string))\n",
    "        return blocks_args\n",
    "\n",
    "    @staticmethod\n",
    "    def encode(blocks_args):\n",
    "        \"\"\"Encode a list of BlockArgs to a list of strings.\n",
    "        Args:\n",
    "            blocks_args (list[namedtuples]): A list of BlockArgs namedtuples of block args.\n",
    "        Returns:\n",
    "            block_strings: A list of strings, each string is a notation of block.\n",
    "        \"\"\"\n",
    "        block_strings = []\n",
    "        for block in blocks_args:\n",
    "            block_strings.append(BlockDecoder._encode_block_string(block))\n",
    "        return block_strings\n",
    "\n",
    "\n",
    "def efficientnet_params(model_name):\n",
    "    \"\"\"Map EfficientNet model name to parameter coefficients.\n",
    "    Args:\n",
    "        model_name (str): Model name to be queried.\n",
    "    Returns:\n",
    "        params_dict[model_name]: A (width,depth,res,dropout) tuple.\n",
    "    \"\"\"\n",
    "    params_dict = {\n",
    "        # Coefficients:   width,depth,res,dropout\n",
    "        'efficientnet-b0': (1.0, 1.0, 224, 0.2),\n",
    "        'efficientnet-b1': (1.0, 1.1, 240, 0.2),\n",
    "        'efficientnet-b2': (1.1, 1.2, 260, 0.3),\n",
    "        'efficientnet-b3': (1.2, 1.4, 300, 0.3),\n",
    "        'efficientnet-b4': (1.4, 1.8, 380, 0.4),\n",
    "        'efficientnet-b5': (1.6, 2.2, 456, 0.4),\n",
    "        'efficientnet-b6': (1.8, 2.6, 528, 0.5),\n",
    "        'efficientnet-b7': (2.0, 3.1, 600, 0.5),\n",
    "        'efficientnet-b8': (2.2, 3.6, 672, 0.5),\n",
    "        'efficientnet-l2': (4.3, 5.3, 800, 0.5),\n",
    "    }\n",
    "    return params_dict[model_name]\n",
    "\n",
    "\n",
    "def efficientnet(width_coefficient=None, depth_coefficient=None, image_size=None,\n",
    "                 dropout_rate=0.2, drop_connect_rate=0.2, num_classes=1000):\n",
    "    \"\"\"Create BlockArgs and GlobalParams for efficientnet model.\n",
    "    Args:\n",
    "        width_coefficient (float)\n",
    "        depth_coefficient (float)\n",
    "        image_size (int)\n",
    "        dropout_rate (float)\n",
    "        drop_connect_rate (float)\n",
    "        num_classes (int)\n",
    "        Meaning as the name suggests.\n",
    "    Returns:\n",
    "        blocks_args, global_params.\n",
    "    \"\"\"\n",
    "\n",
    "    # Blocks args for the whole model(efficientnet-b0 by default)\n",
    "    # It will be modified in the construction of EfficientNet Class according to model\n",
    "    blocks_args = [\n",
    "        'r1_k3_s11_e1_i32_o16_se0.25',\n",
    "        'r2_k3_s22_e6_i16_o24_se0.25',\n",
    "        'r2_k5_s22_e6_i24_o40_se0.25',\n",
    "        'r3_k3_s22_e6_i40_o80_se0.25',\n",
    "        'r3_k5_s11_e6_i80_o112_se0.25',\n",
    "        'r4_k5_s22_e6_i112_o192_se0.25',\n",
    "        'r1_k3_s11_e6_i192_o320_se0.25',\n",
    "    ]\n",
    "    blocks_args = BlockDecoder.decode(blocks_args)\n",
    "\n",
    "    global_params = GlobalParams(\n",
    "        width_coefficient=width_coefficient,\n",
    "        depth_coefficient=depth_coefficient,\n",
    "        image_size=image_size,\n",
    "        dropout_rate=dropout_rate,\n",
    "\n",
    "        num_classes=num_classes,\n",
    "        batch_norm_momentum=0.99,\n",
    "        batch_norm_epsilon=1e-3,\n",
    "        drop_connect_rate=drop_connect_rate,\n",
    "        depth_divisor=8,\n",
    "        min_depth=None,\n",
    "    )\n",
    "\n",
    "    return blocks_args, global_params\n",
    "\n",
    "\n",
    "def get_model_params(model_name, override_params):\n",
    "    \"\"\"Get the block args and global params for a given model name.\n",
    "    Args:\n",
    "        model_name (str): Model's name.\n",
    "        override_params (dict): A dict to modify global_params.\n",
    "    Returns:\n",
    "        blocks_args, global_params\n",
    "    \"\"\"\n",
    "    if model_name.startswith('efficientnet'):\n",
    "        w, d, s, p = efficientnet_params(model_name)\n",
    "        # note: all models have drop connect rate = 0.2\n",
    "        blocks_args, global_params = efficientnet(\n",
    "            width_coefficient=w, depth_coefficient=d, dropout_rate=p, image_size=s)\n",
    "    else:\n",
    "        raise NotImplementedError('model name is not pre-defined: %s' % model_name)\n",
    "    if override_params:\n",
    "        # ValueError will be raised here if override_params has fields not included in global_params.\n",
    "        global_params = global_params._replace(**override_params)\n",
    "    return blocks_args, global_params\n",
    "\n",
    "\n",
    "# train with Standard methods\n",
    "# check more details in paper(EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks)\n",
    "url_map = {\n",
    "    'efficientnet-b0': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth',\n",
    "    'efficientnet-b1': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b1-f1951068.pth',\n",
    "    'efficientnet-b2': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b2-8bb594d6.pth',\n",
    "    'efficientnet-b3': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b3-5fb5a3c3.pth',\n",
    "    'efficientnet-b4': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b4-6ed6700e.pth',\n",
    "    'efficientnet-b5': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b5-b6417697.pth',\n",
    "    'efficientnet-b6': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b6-c76e70fd.pth',\n",
    "    'efficientnet-b7': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b7-dcc49843.pth',\n",
    "}\n",
    "\n",
    "# train with Adversarial Examples(AdvProp)\n",
    "# check more details in paper(Adversarial Examples Improve Image Recognition)\n",
    "url_map_advprop = {\n",
    "    'efficientnet-b0': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/adv-efficientnet-b0-b64d5a18.pth',\n",
    "    'efficientnet-b1': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/adv-efficientnet-b1-0f3ce85a.pth',\n",
    "    'efficientnet-b2': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/adv-efficientnet-b2-6e9d97e5.pth',\n",
    "    'efficientnet-b3': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/adv-efficientnet-b3-cdd7c0f4.pth',\n",
    "    'efficientnet-b4': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/adv-efficientnet-b4-44fb3a87.pth',\n",
    "    'efficientnet-b5': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/adv-efficientnet-b5-86493f6b.pth',\n",
    "    'efficientnet-b6': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/adv-efficientnet-b6-ac80338e.pth',\n",
    "    'efficientnet-b7': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/adv-efficientnet-b7-4652b6dd.pth',\n",
    "    'efficientnet-b8': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/adv-efficientnet-b8-22a8fe65.pth',\n",
    "}\n",
    "\n",
    "# TODO: add the petrained weights url map of 'efficientnet-l2'\n",
    "\n",
    "\n",
    "def load_pretrained_weights(model, model_name, weights_path=None, load_fc=True, advprop=False):\n",
    "    \"\"\"Loads pretrained weights from weights path or download using url.\n",
    "    Args:\n",
    "        model (Module): The whole model of efficientnet.\n",
    "        model_name (str): Model name of efficientnet.\n",
    "        weights_path (None or str): \n",
    "            str: path to pretrained weights file on the local disk.\n",
    "            None: use pretrained weights downloaded from the Internet.\n",
    "        load_fc (bool): Whether to load pretrained weights for fc layer at the end of the model.\n",
    "        advprop (bool): Whether to load pretrained weights\n",
    "                        trained with advprop (valid when weights_path is None).\n",
    "    \"\"\"\n",
    "    if isinstance(weights_path,str):\n",
    "        state_dict = torch.load(weights_path)\n",
    "    else:\n",
    "        # AutoAugment or Advprop (different preprocessing)\n",
    "        url_map_ = url_map_advprop if advprop else url_map\n",
    "        state_dict = model_zoo.load_url(url_map_[model_name])\n",
    "    \n",
    "    if load_fc:\n",
    "        ret = model.load_state_dict(state_dict, strict=False)\n",
    "        assert not ret.missing_keys, f'Missing keys when loading pretrained weights: {ret.missing_keys}'\n",
    "    else:\n",
    "        state_dict.pop('_fc.weight')\n",
    "        state_dict.pop('_fc.bias')\n",
    "        ret = model.load_state_dict(state_dict, strict=False)\n",
    "        assert set(ret.missing_keys) == set(\n",
    "            ['_fc.weight', '_fc.bias']), f'Missing keys when loading pretrained weights: {ret.missing_keys}'\n",
    "    assert not ret.unexpected_keys, f'Missing keys when loading pretrained weights: {ret.unexpected_keys}'\n",
    "\n",
    "    print('Loaded pretrained weights for {}'.format(model_name))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "VALID_MODELS = (\n",
    "    'efficientnet-b0', 'efficientnet-b1', 'efficientnet-b2', 'efficientnet-b3',\n",
    "    'efficientnet-b4', 'efficientnet-b5', 'efficientnet-b6', 'efficientnet-b7',\n",
    "    'efficientnet-b8',\n",
    "\n",
    "    # Support the construction of 'efficientnet-l2' without pretrained weights\n",
    "    'efficientnet-l2'\n",
    ")\n",
    "\n",
    "\n",
    "class MBConvBlock(nn.Module):\n",
    "    \"\"\"Mobile Inverted Residual Bottleneck Block.\n",
    "    Args:\n",
    "        block_args (namedtuple): BlockArgs, defined in utils.py.\n",
    "        global_params (namedtuple): GlobalParam, defined in utils.py.\n",
    "        image_size (tuple or list): [image_height, image_width].\n",
    "    References:\n",
    "        [1] https://arxiv.org/abs/1704.04861 (MobileNet v1)\n",
    "        [2] https://arxiv.org/abs/1801.04381 (MobileNet v2)\n",
    "        [3] https://arxiv.org/abs/1905.02244 (MobileNet v3)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, block_args, global_params, image_size=None):\n",
    "        super().__init__()\n",
    "        self._block_args = block_args\n",
    "        self._bn_mom = 1 - global_params.batch_norm_momentum # pytorch's difference from tensorflow\n",
    "        self._bn_eps = global_params.batch_norm_epsilon\n",
    "        self.has_se = (self._block_args.se_ratio is not None) and (0 < self._block_args.se_ratio <= 1)\n",
    "        self.id_skip = block_args.id_skip  # whether to use skip connection and drop connect\n",
    "\n",
    "        # Expansion phase (Inverted Bottleneck)\n",
    "        inp = self._block_args.input_filters  # number of input channels\n",
    "        oup = self._block_args.input_filters * self._block_args.expand_ratio  # number of output channels\n",
    "        if self._block_args.expand_ratio != 1:\n",
    "            Conv2d = get_same_padding_conv2d(image_size=image_size)\n",
    "            self._expand_conv = Conv2d(in_channels=inp, out_channels=oup, kernel_size=1, bias=False)\n",
    "            self._bn0 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "            # image_size = calculate_output_image_size(image_size, 1) <-- this wouldn't modify image_size\n",
    "\n",
    "        # Depthwise convolution phase\n",
    "        k = self._block_args.kernel_size\n",
    "        s = self._block_args.stride\n",
    "        Conv2d = get_same_padding_conv2d(image_size=image_size)\n",
    "        self._depthwise_conv = Conv2d(\n",
    "            in_channels=oup, out_channels=oup, groups=oup,  # groups makes it depthwise\n",
    "            kernel_size=k, stride=s, bias=False)\n",
    "        self._bn1 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "        image_size = calculate_output_image_size(image_size, s)\n",
    "\n",
    "        # Squeeze and Excitation layer, if desired\n",
    "        if self.has_se:\n",
    "            Conv2d = get_same_padding_conv2d(image_size=(1,1))\n",
    "            num_squeezed_channels = max(1, int(self._block_args.input_filters * self._block_args.se_ratio))\n",
    "            self._se_reduce = Conv2d(in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1)\n",
    "            self._se_expand = Conv2d(in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1)\n",
    "\n",
    "        # Pointwise convolution phase\n",
    "        final_oup = self._block_args.output_filters\n",
    "        Conv2d = get_same_padding_conv2d(image_size=image_size)\n",
    "        self._project_conv = Conv2d(in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False)\n",
    "        self._bn2 = nn.BatchNorm2d(num_features=final_oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "        self._swish = MemoryEfficientSwish()\n",
    "\n",
    "    def forward(self, inputs, drop_connect_rate=None):\n",
    "        \"\"\"MBConvBlock's forward function.\n",
    "        Args:\n",
    "            inputs (tensor): Input tensor.\n",
    "            drop_connect_rate (bool): Drop connect rate (float, between 0 and 1).\n",
    "        Returns:\n",
    "            Output of this block after processing.\n",
    "        \"\"\"\n",
    "\n",
    "        # Expansion and Depthwise Convolution\n",
    "        x = inputs\n",
    "        if self._block_args.expand_ratio != 1:\n",
    "            x = self._expand_conv(inputs)\n",
    "            x = self._bn0(x)\n",
    "            x = self._swish(x)\n",
    "\n",
    "        x = self._depthwise_conv(x)\n",
    "        x = self._bn1(x)\n",
    "        x = self._swish(x)\n",
    "\n",
    "        # Squeeze and Excitation\n",
    "        if self.has_se:\n",
    "            x_squeezed = F.adaptive_avg_pool2d(x, 1)\n",
    "            x_squeezed = self._se_reduce(x_squeezed)\n",
    "            x_squeezed = self._swish(x_squeezed)\n",
    "            x_squeezed = self._se_expand(x_squeezed)\n",
    "            x = torch.sigmoid(x_squeezed) * x\n",
    "\n",
    "        # Pointwise Convolution\n",
    "        x = self._project_conv(x)\n",
    "        x = self._bn2(x)\n",
    "\n",
    "        # Skip connection and drop connect\n",
    "        input_filters, output_filters = self._block_args.input_filters, self._block_args.output_filters\n",
    "        if self.id_skip and self._block_args.stride == 1 and input_filters == output_filters:\n",
    "            # The combination of skip connection and drop connect brings about stochastic depth.\n",
    "            if drop_connect_rate:\n",
    "                x = drop_connect(x, p=drop_connect_rate, training=self.training)\n",
    "            x = x + inputs  # skip connection\n",
    "        return x\n",
    "\n",
    "    def set_swish(self, memory_efficient=True):\n",
    "        \"\"\"Sets swish function as memory efficient (for training) or standard (for export).\n",
    "        Args:\n",
    "            memory_efficient (bool): Whether to use memory-efficient version of swish.\n",
    "        \"\"\"\n",
    "        self._swish = MemoryEfficientSwish() if memory_efficient else Swish()\n",
    "\n",
    "\n",
    "class EfficientNet(nn.Module):\n",
    "    \"\"\"EfficientNet model.\n",
    "       Most easily loaded with the .from_name or .from_pretrained methods.\n",
    "    Args:\n",
    "        blocks_args (list[namedtuple]): A list of BlockArgs to construct blocks.\n",
    "        global_params (namedtuple): A set of GlobalParams shared between blocks.\n",
    "    \n",
    "    References:\n",
    "        [1] https://arxiv.org/abs/1905.11946 (EfficientNet)\n",
    "    Example:\n",
    "        >>> import torch\n",
    "        >>> from efficientnet.model import EfficientNet\n",
    "        >>> inputs = torch.rand(1, 3, 224, 224)\n",
    "        >>> model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "        >>> model.eval()\n",
    "        >>> outputs = model(inputs)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, blocks_args=None, global_params=None):\n",
    "        super().__init__()\n",
    "        assert isinstance(blocks_args, list), 'blocks_args should be a list'\n",
    "        assert len(blocks_args) > 0, 'block args must be greater than 0'\n",
    "        self._global_params = global_params\n",
    "        self._blocks_args = blocks_args\n",
    "\n",
    "        # Batch norm parameters\n",
    "        bn_mom = 1 - self._global_params.batch_norm_momentum\n",
    "        bn_eps = self._global_params.batch_norm_epsilon\n",
    "\n",
    "        # Get stem static or dynamic convolution depending on image size\n",
    "        image_size = global_params.image_size\n",
    "        Conv2d = get_same_padding_conv2d(image_size=image_size)\n",
    "\n",
    "        # Stem\n",
    "        in_channels = 3  # rgb\n",
    "        out_channels = round_filters(32, self._global_params)  # number of output channels\n",
    "        self._conv_stem = Conv2d(in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n",
    "        self._bn0 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n",
    "        image_size = calculate_output_image_size(image_size, 2)\n",
    "\n",
    "        # Build blocks\n",
    "        self._blocks = nn.ModuleList([])\n",
    "        for block_args in self._blocks_args:\n",
    "\n",
    "            # Update block input and output filters based on depth multiplier.\n",
    "            block_args = block_args._replace(\n",
    "                input_filters=round_filters(block_args.input_filters, self._global_params),\n",
    "                output_filters=round_filters(block_args.output_filters, self._global_params),\n",
    "                num_repeat=round_repeats(block_args.num_repeat, self._global_params)\n",
    "            )\n",
    "\n",
    "            # The first block needs to take care of stride and filter size increase.\n",
    "            self._blocks.append(MBConvBlock(block_args, self._global_params, image_size=image_size))\n",
    "            image_size = calculate_output_image_size(image_size, block_args.stride)\n",
    "            if block_args.num_repeat > 1: # modify block_args to keep same output size\n",
    "                block_args = block_args._replace(input_filters=block_args.output_filters, stride=1)\n",
    "            for _ in range(block_args.num_repeat - 1):\n",
    "                self._blocks.append(MBConvBlock(block_args, self._global_params, image_size=image_size))\n",
    "                # image_size = calculate_output_image_size(image_size, block_args.stride)  # stride = 1\n",
    "\n",
    "        # Head\n",
    "        in_channels = block_args.output_filters  # output of final block\n",
    "        out_channels = round_filters(1280, self._global_params)\n",
    "        Conv2d = get_same_padding_conv2d(image_size=image_size)\n",
    "        self._conv_head = Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self._bn1 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n",
    "\n",
    "        # Final linear layer\n",
    "        self._avg_pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self._dropout = nn.Dropout(self._global_params.dropout_rate)\n",
    "        self._fc = nn.Linear(out_channels, self._global_params.num_classes)\n",
    "        self._swish = MemoryEfficientSwish()\n",
    "\n",
    "    def set_swish(self, memory_efficient=True):\n",
    "        \"\"\"Sets swish function as memory efficient (for training) or standard (for export).\n",
    "        Args:\n",
    "            memory_efficient (bool): Whether to use memory-efficient version of swish.\n",
    "        \"\"\"\n",
    "        self._swish = MemoryEfficientSwish() if memory_efficient else Swish()\n",
    "        for block in self._blocks:\n",
    "            block.set_swish(memory_efficient)\n",
    "\n",
    "    def extract_endpoints(self, inputs):\n",
    "        \"\"\"Use convolution layer to extract features\n",
    "        from reduction levels i in [1, 2, 3, 4, 5].\n",
    "        Args:\n",
    "            inputs (tensor): Input tensor.\n",
    "        Returns:\n",
    "            Dictionary of last intermediate features\n",
    "            with reduction levels i in [1, 2, 3, 4, 5].\n",
    "            Example:\n",
    "                >>> import torch\n",
    "                >>> from efficientnet.model import EfficientNet\n",
    "                >>> inputs = torch.rand(1, 3, 224, 224)\n",
    "                >>> model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "                >>> endpoints = model.extract_endpoints(inputs)\n",
    "                >>> print(endpoints['reduction_1'].shape)  # torch.Size([1, 16, 112, 112])\n",
    "                >>> print(endpoints['reduction_2'].shape)  # torch.Size([1, 24, 56, 56])\n",
    "                >>> print(endpoints['reduction_3'].shape)  # torch.Size([1, 40, 28, 28])\n",
    "                >>> print(endpoints['reduction_4'].shape)  # torch.Size([1, 112, 14, 14])\n",
    "                >>> print(endpoints['reduction_5'].shape)  # torch.Size([1, 1280, 7, 7])\n",
    "        \"\"\"\n",
    "        endpoints = dict()\n",
    "\n",
    "        # Stem\n",
    "        x = self._swish(self._bn0(self._conv_stem(inputs)))\n",
    "        prev_x = x\n",
    "\n",
    "        # Blocks\n",
    "        for idx, block in enumerate(self._blocks):\n",
    "            drop_connect_rate = self._global_params.drop_connect_rate\n",
    "            if drop_connect_rate:\n",
    "                drop_connect_rate *= float(idx) / len(self._blocks) # scale drop connect_rate\n",
    "            x = block(x, drop_connect_rate=drop_connect_rate)\n",
    "            if prev_x.size(2) > x.size(2):\n",
    "                endpoints[f'reduction_{len(endpoints)+1}'] = prev_x\n",
    "            prev_x = x\n",
    "\n",
    "        # Head\n",
    "        x = self._swish(self._bn1(self._conv_head(x)))\n",
    "        endpoints[f'reduction_{len(endpoints)+1}'] = x\n",
    "\n",
    "        return endpoints\n",
    "\n",
    "    def extract_features(self, inputs):\n",
    "        \"\"\"use convolution layer to extract feature .\n",
    "        Args:\n",
    "            inputs (tensor): Input tensor.\n",
    "        Returns:\n",
    "            Output of the final convolution \n",
    "            layer in the efficientnet model.\n",
    "        \"\"\"\n",
    "        # Stem\n",
    "        x = self._swish(self._bn0(self._conv_stem(inputs)))\n",
    "\n",
    "        # Blocks\n",
    "        for idx, block in enumerate(self._blocks):\n",
    "            drop_connect_rate = self._global_params.drop_connect_rate\n",
    "            if drop_connect_rate:\n",
    "                drop_connect_rate *= float(idx) / len(self._blocks) # scale drop connect_rate\n",
    "            x = block(x, drop_connect_rate=drop_connect_rate)\n",
    "        \n",
    "        # Head\n",
    "        x = self._swish(self._bn1(self._conv_head(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"EfficientNet's forward function.\n",
    "           Calls extract_features to extract features, applies final linear layer, and returns logits.\n",
    "        Args:\n",
    "            inputs (tensor): Input tensor.\n",
    "        Returns:\n",
    "            Output of this model after processing.\n",
    "        \"\"\"\n",
    "        # Convolution layers\n",
    "        x = self.extract_features(inputs)\n",
    "\n",
    "        # Pooling and final linear layer\n",
    "        x = self._avg_pooling(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self._dropout(x)\n",
    "        x = self._fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @classmethod\n",
    "    def from_name(cls, model_name, in_channels=3, **override_params):\n",
    "        \"\"\"create an efficientnet model according to name.\n",
    "        Args:\n",
    "            model_name (str): Name for efficientnet.\n",
    "            in_channels (int): Input data's channel number.\n",
    "            override_params (other key word params): \n",
    "                Params to override model's global_params.\n",
    "                Optional key:\n",
    "                    'width_coefficient', 'depth_coefficient',\n",
    "                    'image_size', 'dropout_rate',\n",
    "                    'num_classes', 'batch_norm_momentum',\n",
    "                    'batch_norm_epsilon', 'drop_connect_rate',\n",
    "                    'depth_divisor', 'min_depth'\n",
    "        Returns:\n",
    "            An efficientnet model.\n",
    "        \"\"\"\n",
    "        cls._check_model_name_is_valid(model_name)\n",
    "        blocks_args, global_params = get_model_params(model_name, override_params)\n",
    "        model = cls(blocks_args, global_params)\n",
    "        model._change_in_channels(in_channels)\n",
    "        return model\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_name, weights_path=None, advprop=False, \n",
    "                        in_channels=3, num_classes=1000, **override_params):\n",
    "        \"\"\"create an efficientnet model according to name.\n",
    "        Args:\n",
    "            model_name (str): Name for efficientnet.\n",
    "            weights_path (None or str): \n",
    "                str: path to pretrained weights file on the local disk.\n",
    "                None: use pretrained weights downloaded from the Internet.\n",
    "            advprop (bool): \n",
    "                Whether to load pretrained weights\n",
    "                trained with advprop (valid when weights_path is None).\n",
    "            in_channels (int): Input data's channel number.\n",
    "            num_classes (int): \n",
    "                Number of categories for classification.\n",
    "                It controls the output size for final linear layer.\n",
    "            override_params (other key word params): \n",
    "                Params to override model's global_params.\n",
    "                Optional key:\n",
    "                    'width_coefficient', 'depth_coefficient',\n",
    "                    'image_size', 'dropout_rate',\n",
    "                    'num_classes', 'batch_norm_momentum',\n",
    "                    'batch_norm_epsilon', 'drop_connect_rate',\n",
    "                    'depth_divisor', 'min_depth'\n",
    "        Returns:\n",
    "            A pretrained efficientnet model.\n",
    "        \"\"\"\n",
    "        model = cls.from_name(model_name, num_classes = num_classes, **override_params)\n",
    "        load_pretrained_weights(model, model_name, weights_path=weights_path, load_fc=(num_classes == 1000), advprop=advprop)\n",
    "        model._change_in_channels(in_channels)\n",
    "        return model\n",
    "\n",
    "    @classmethod\n",
    "    def get_image_size(cls, model_name):\n",
    "        \"\"\"Get the input image size for a given efficientnet model.\n",
    "        Args:\n",
    "            model_name (str): Name for efficientnet.\n",
    "        Returns:\n",
    "            Input image size (resolution).\n",
    "        \"\"\"\n",
    "        cls._check_model_name_is_valid(model_name)\n",
    "        _, _, res, _ = efficientnet_params(model_name)\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def _check_model_name_is_valid(cls, model_name):\n",
    "        \"\"\"Validates model name. \n",
    "        Args:\n",
    "            model_name (str): Name for efficientnet.\n",
    "        Returns:\n",
    "            bool: Is a valid name or not.\n",
    "        \"\"\"\n",
    "        if model_name not in VALID_MODELS:\n",
    "            raise ValueError('model_name should be one of: ' + ', '.join(VALID_MODELS))\n",
    "\n",
    "    def _change_in_channels(self, in_channels):\n",
    "        \"\"\"Adjust model's first convolution layer to in_channels, if in_channels not equals 3.\n",
    "        Args:\n",
    "            in_channels (int): Input data's channel number.\n",
    "        \"\"\"\n",
    "        if in_channels != 3:\n",
    "            Conv2d = get_same_padding_conv2d(image_size = self._global_params.image_size)\n",
    "            out_channels = round_filters(32, self._global_params)\n",
    "            self._conv_stem = Conv2d(in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "InPlaceABN = None\n",
    "\n",
    "\n",
    "class Conv2dReLU(nn.Sequential):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            padding=0,\n",
    "            stride=1,\n",
    "            use_batchnorm=True,\n",
    "    ):\n",
    "\n",
    "        if use_batchnorm == \"inplace\" and InPlaceABN is None:\n",
    "            raise RuntimeError(\n",
    "                \"In order to use `use_batchnorm='inplace'` inplace_abn package must be installed. \"\n",
    "                + \"To install see: https://github.com/mapillary/inplace_abn\"\n",
    "            )\n",
    "\n",
    "        conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=not (use_batchnorm),\n",
    "        )\n",
    "        relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        if use_batchnorm == \"inplace\":\n",
    "            bn = InPlaceABN(out_channels, activation=\"leaky_relu\", activation_param=0.0)\n",
    "            relu = nn.Identity()\n",
    "\n",
    "        elif use_batchnorm and use_batchnorm != \"inplace\":\n",
    "            bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        else:\n",
    "            bn = nn.Identity()\n",
    "\n",
    "        super(Conv2dReLU, self).__init__(conv, bn, relu)\n",
    "\n",
    "\n",
    "class SCSEModule(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.cSE = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, in_channels // reduction, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels // reduction, in_channels, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.sSE = nn.Sequential(nn.Conv2d(in_channels, 1, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.cSE(x) + x * self.sSE(x)\n",
    "\n",
    "\n",
    "class ArgMax(nn.Module):\n",
    "\n",
    "    def __init__(self, dim=None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.argmax(x, dim=dim)\n",
    "\n",
    "\n",
    "class Activation(nn.Module):\n",
    "\n",
    "    def __init__(self, name, **params):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        if name is None or name == 'identity':\n",
    "            self.activation = nn.Identity(**params)\n",
    "        elif name == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif name == 'softmax2d':\n",
    "            self.activation = nn.Softmax(dim=1, **params)\n",
    "        elif name == 'softmax':\n",
    "            self.activation = nn.Softmax(**params)\n",
    "        elif name == 'logsoftmax':\n",
    "            self.activation = nn.LogSoftmax(**params)\n",
    "        elif name == 'argmax':\n",
    "            self.activation = ArgMax(**params)\n",
    "        elif name == 'argmax2d':\n",
    "            self.activation = ArgMax(dim=1, **params)\n",
    "        elif callable(name):\n",
    "            self.activation = name(**params)\n",
    "        else:\n",
    "            raise ValueError('Activation should be callable/sigmoid/softmax/logsoftmax/None; got {}'.format(name))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, name, **params):\n",
    "        super().__init__()\n",
    "\n",
    "        if name is None:\n",
    "            self.attention = nn.Identity(**params)\n",
    "        elif name == 'scse':\n",
    "            self.attention = SCSEModule(**params)\n",
    "        else:\n",
    "            raise ValueError(\"Attention {} is not implemented\".format(name))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.attention(x)\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            skip_channels,\n",
    "            out_channels,\n",
    "            use_batchnorm=True,\n",
    "            attention_type=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2dReLU(\n",
    "            in_channels + skip_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "        self.attention1 = Attention(attention_type, in_channels=in_channels + skip_channels)\n",
    "        self.conv2 = Conv2dReLU(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "        self.attention2 = Attention(attention_type, in_channels=out_channels)\n",
    "\n",
    "    def forward(self, x, skip=None):\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        if skip is not None:\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "            x = self.attention1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.attention2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CenterBlock(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, use_batchnorm=True):\n",
    "        conv1 = Conv2dReLU(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "        conv2 = Conv2dReLU(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "        super().__init__(conv1, conv2)\n",
    "\n",
    "\n",
    "class UnetDecoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            encoder_channels,\n",
    "            decoder_channels,\n",
    "            n_blocks=5,\n",
    "            use_batchnorm=True,\n",
    "            attention_type=None,\n",
    "            center=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if n_blocks != len(decoder_channels):\n",
    "            raise ValueError(\n",
    "                \"Model depth is {}, but you provide `decoder_channels` for {} blocks.\".format(\n",
    "                    n_blocks, len(decoder_channels)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        encoder_channels = encoder_channels[1:]  # remove first skip with same spatial resolution\n",
    "        encoder_channels = encoder_channels[::-1]  # reverse channels to start from head of encoder\n",
    "\n",
    "        # computing blocks input and output channels\n",
    "        head_channels = encoder_channels[0]\n",
    "        in_channels = [head_channels] + list(decoder_channels[:-1])\n",
    "        skip_channels = list(encoder_channels[1:]) + [0]\n",
    "        out_channels = decoder_channels\n",
    "\n",
    "        if center:\n",
    "            self.center = CenterBlock(\n",
    "                head_channels, head_channels, use_batchnorm=use_batchnorm\n",
    "            )\n",
    "        else:\n",
    "            self.center = nn.Identity()\n",
    "\n",
    "        # combine decoder keyword arguments\n",
    "        kwargs = dict(use_batchnorm=use_batchnorm, attention_type=attention_type)\n",
    "        blocks = [\n",
    "            DecoderBlock(in_ch, skip_ch, out_ch, **kwargs)\n",
    "            for in_ch, skip_ch, out_ch in zip(in_channels, skip_channels, out_channels)\n",
    "        ]\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "\n",
    "    def forward(self, *features):\n",
    "\n",
    "        features = features[1:]    # remove first skip with same spatial resolution\n",
    "        features = features[::-1]  # reverse channels to start from head of encoder\n",
    "\n",
    "        head = features[0]\n",
    "        skips = features[1:]\n",
    "\n",
    "        x = self.center(head)\n",
    "        for i, decoder_block in enumerate(self.blocks):\n",
    "            skip = skips[i] if i < len(skips) else None\n",
    "            x = decoder_block(x, skip)\n",
    "\n",
    "        return x\n",
    "\n",
    "def patch_first_conv(model, in_channels):\n",
    "    \"\"\"Change first convolution layer input channels.\n",
    "    In case:\n",
    "        in_channels == 1 or in_channels == 2 -> reuse original weights\n",
    "        in_channels > 3 -> make random kaiming normal initialization\n",
    "    \"\"\"\n",
    "\n",
    "    # get first conv\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            break\n",
    "\n",
    "    # change input channels for first conv\n",
    "    module.in_channels = in_channels\n",
    "    weight = module.weight.detach()\n",
    "    reset = False\n",
    "\n",
    "    if in_channels == 1:\n",
    "        weight = weight.sum(1, keepdim=True)\n",
    "    elif in_channels == 2:\n",
    "        weight = weight[:, :2] * (3.0 / 2.0)\n",
    "    else:\n",
    "        reset = True\n",
    "        weight = torch.Tensor(\n",
    "            module.out_channels,\n",
    "            module.in_channels // module.groups,\n",
    "            *module.kernel_size\n",
    "        )\n",
    "\n",
    "    module.weight = nn.parameter.Parameter(weight)\n",
    "    if reset:\n",
    "        module.reset_parameters()\n",
    "\n",
    "\n",
    "def replace_strides_with_dilation(module, dilation_rate):\n",
    "    \"\"\"Patch Conv2d modules replacing strides with dilation\"\"\"\n",
    "    for mod in module.modules():\n",
    "        if isinstance(mod, nn.Conv2d):\n",
    "            mod.stride = (1, 1)\n",
    "            mod.dilation = (dilation_rate, dilation_rate)\n",
    "            kh, kw = mod.kernel_size\n",
    "            mod.padding = ((kh // 2) * dilation_rate, (kh // 2) * dilation_rate)\n",
    "\n",
    "            # Kostyl for EfficientNet\n",
    "            if hasattr(mod, \"static_padding\"):\n",
    "                mod.static_padding = nn.Identity()\n",
    "                \n",
    "class EncoderMixin:\n",
    "    \"\"\"Add encoder functionality such as:\n",
    "        - output channels specification of feature tensors (produced by encoder)\n",
    "        - patching first convolution for arbitrary input channels\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def out_channels(self):\n",
    "        \"\"\"Return channels dimensions for each tensor of forward output of encoder\"\"\"\n",
    "        return self._out_channels[: self._depth + 1]\n",
    "\n",
    "    def set_in_channels(self, in_channels):\n",
    "        \"\"\"Change first convolution chennels\"\"\"\n",
    "        if in_channels == 3:\n",
    "            return\n",
    "\n",
    "        self._in_channels = in_channels\n",
    "        if self._out_channels[0] == 3:\n",
    "            self._out_channels = tuple([in_channels] + list(self._out_channels)[1:])\n",
    "\n",
    "        patch_first_conv(model=self, in_channels=in_channels)\n",
    "\n",
    "    def get_stages(self):\n",
    "        \"\"\"Method should be overridden in encoder\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def make_dilated(self, stage_list, dilation_list):\n",
    "        stages = self.get_stages()\n",
    "        for stage_indx, dilation_rate in zip(stage_list, dilation_list):\n",
    "            replace_strides_with_dilation(\n",
    "                module=stages[stage_indx],\n",
    "                dilation_rate=dilation_rate,\n",
    "            )\n",
    "            \n",
    "def preprocess_input(\n",
    "    x, mean=None, std=None, input_space=\"RGB\", input_range=None, **kwargs\n",
    "):\n",
    "\n",
    "    if input_space == \"BGR\":\n",
    "        x = x[..., ::-1].copy()\n",
    "\n",
    "    if input_range is not None:\n",
    "        if x.max() > 1 and input_range[1] == 1:\n",
    "            x = x / 255.0\n",
    "\n",
    "    if mean is not None:\n",
    "        mean = np.array(mean)\n",
    "        x = x - mean\n",
    "\n",
    "    if std is not None:\n",
    "        std = np.array(std)\n",
    "        x = x / std\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\" Each encoder should have following attributes and methods and be inherited from `_base.EncoderMixin`\n",
    "Attributes:\n",
    "    _out_channels (list of int): specify number of channels for each encoder feature tensor\n",
    "    _depth (int): specify number of stages in decoder (in other words number of downsampling operations)\n",
    "    _in_channels (int): default number of input channels in first Conv2d layer for encoder (usually 3)\n",
    "Methods:\n",
    "    forward(self, x: torch.Tensor)\n",
    "        produce list of features of different spatial resolutions, each feature is a 4D torch.tensor of\n",
    "        shape NCHW (features should be sorted in descending order according to spatial resolution, starting\n",
    "        with resolution same as input `x` tensor).\n",
    "        Input: `x` with shape (1, 3, 64, 64)\n",
    "        Output: [f0, f1, f2, f3, f4, f5] - features with corresponding shapes\n",
    "                [(1, 3, 64, 64), (1, 64, 32, 32), (1, 128, 16, 16), (1, 256, 8, 8),\n",
    "                (1, 512, 4, 4), (1, 1024, 2, 2)] (C - dim may differ)\n",
    "        also should support number of features according to specified depth, e.g. if depth = 5,\n",
    "        number of feature tensors = 6 (one with same resolution as input and 5 downsampled),\n",
    "        depth = 3 -> number of feature tensors = 4 (one with same resolution as input and 3 downsampled).\n",
    "\"\"\"\n",
    "#import torch.nn as nn\n",
    "#from efficientnet_pytorch import EfficientNet\n",
    "#from efficientnet_pytorch.utils import url_map, url_map_advprop, get_model_params\n",
    "\n",
    "#from ._base import EncoderMixin\n",
    "\n",
    "\n",
    "class EfficientNetEncoder(EfficientNet, EncoderMixin):\n",
    "    def __init__(self, stage_idxs, out_channels, model_name, depth=5):\n",
    "\n",
    "        blocks_args, global_params = get_model_params(model_name, override_params=None)\n",
    "        super().__init__(blocks_args, global_params)\n",
    "\n",
    "        self._stage_idxs = stage_idxs\n",
    "        self._out_channels = out_channels\n",
    "        self._depth = depth\n",
    "        self._in_channels = 3\n",
    "        \n",
    "        # remove mid blocks\n",
    "        del self._blocks[15]\n",
    "        del self._blocks[14]\n",
    "        del self._blocks[13]\n",
    "\n",
    "        # remove the head\n",
    "        del self._avg_pooling\n",
    "        del self._dropout\n",
    "        del self._swish\n",
    "        del self._fc\n",
    "\n",
    "    def get_stages(self):\n",
    "        return [\n",
    "            nn.Identity(),\n",
    "            #nn.Sequential(self._conv_stem, self._bn0, self._swish),\n",
    "            nn.Sequential(self._conv_stem, self._bn0),\n",
    "            self._blocks[:self._stage_idxs[0]],\n",
    "            self._blocks[self._stage_idxs[0]:self._stage_idxs[1]],\n",
    "            self._blocks[self._stage_idxs[1]:self._stage_idxs[2]],\n",
    "            self._blocks[self._stage_idxs[2]:],\n",
    "        ]\n",
    "\n",
    "    def forward(self, x):\n",
    "        stages = self.get_stages()\n",
    "\n",
    "        block_number = 0.\n",
    "        drop_connect_rate = self._global_params.drop_connect_rate\n",
    "\n",
    "        features = []\n",
    "        for i in range(self._depth + 1):\n",
    "\n",
    "            # Identity and Sequential stages\n",
    "            if i < 2:\n",
    "                x = stages[i](x)\n",
    "\n",
    "            # Block stages need drop_connect rate\n",
    "            else:\n",
    "                for module in stages[i]:\n",
    "                    drop_connect = drop_connect_rate * block_number / len(self._blocks)\n",
    "                    block_number += 1.\n",
    "                    x = module(x, drop_connect)\n",
    "\n",
    "            features.append(x)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def load_state_dict(self, state_dict, **kwargs):\n",
    "        state_dict.pop(\"_fc.bias\")\n",
    "        state_dict.pop(\"_fc.weight\")\n",
    "        super().load_state_dict(state_dict, **kwargs)\n",
    "\n",
    "\n",
    "def _get_pretrained_settings(encoder):\n",
    "    pretrained_settings = {\n",
    "        \"imagenet\": {\n",
    "            \"mean\": [0.485, 0.456, 0.406],\n",
    "            \"std\": [0.229, 0.224, 0.225],\n",
    "            \"url\": url_map[encoder],\n",
    "            \"input_space\": \"RGB\",\n",
    "            \"input_range\": [0, 1],\n",
    "        },\n",
    "        \"advprop\": {\n",
    "            \"mean\": [0.5, 0.5, 0.5],\n",
    "            \"std\": [0.5, 0.5, 0.5],\n",
    "            \"url\": url_map_advprop[encoder],\n",
    "            \"input_space\": \"RGB\",\n",
    "            \"input_range\": [0, 1],\n",
    "        }\n",
    "    }\n",
    "    return pretrained_settings\n",
    "\n",
    "\n",
    "efficient_net_encoders = {\n",
    "    \"efficientnet-b0\": {\n",
    "        \"encoder\": EfficientNetEncoder,\n",
    "        \"pretrained_settings\": _get_pretrained_settings(\"efficientnet-b0\"),\n",
    "        \"params\": {\n",
    "            \"out_channels\": (3, 32, 24, 40, 112, 320),\n",
    "            \"stage_idxs\": (3, 5, 9, 16),\n",
    "            \"model_name\": \"efficientnet-b0\",\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "encoders = {}\n",
    "encoders.update(efficient_net_encoders)\n",
    "\n",
    "def get_encoder(name, in_channels=3, depth=5, weights=None):\n",
    "    enc = enetv2(10,10,0)\n",
    "    \n",
    "    inputs = NUMBER_OF_INPUT_LAYERS * 32\n",
    "    \n",
    "    enc.enet._conv_stem = nn.Conv2d(inputs, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "    return enc\n",
    "\n",
    "def get_encoder2(name, in_channels=3, depth=5, weights=None):\n",
    "    Encoder = encoders[name][\"encoder\"]\n",
    "    params = encoders[name][\"params\"]\n",
    "    params.update(depth=depth)\n",
    "    encoder = Encoder(**params)\n",
    "\n",
    "    if weights is not None:\n",
    "        settings = encoders[name][\"pretrained_settings\"][weights]\n",
    "        encoder.load_state_dict(model_zoo.load_url(settings[\"url\"]))\n",
    "\n",
    "    encoder.set_in_channels(in_channels)\n",
    "\n",
    "    print(encoder)\n",
    "    return encoder\n",
    "\n",
    "\n",
    "def get_encoder_names():\n",
    "    return list(encoders.keys())\n",
    "\n",
    "\n",
    "def get_preprocessing_params(encoder_name, pretrained=\"imagenet\"):\n",
    "    settings = encoders[encoder_name][\"pretrained_settings\"]\n",
    "\n",
    "    if pretrained not in settings.keys():\n",
    "        raise ValueError(\"Avaliable pretrained options {}\".format(settings.keys()))\n",
    "\n",
    "    formatted_settings = {}\n",
    "    formatted_settings[\"input_space\"] = settings[pretrained].get(\"input_space\")\n",
    "    formatted_settings[\"input_range\"] = settings[pretrained].get(\"input_range\")\n",
    "    formatted_settings[\"mean\"] = settings[pretrained].get(\"mean\")\n",
    "    formatted_settings[\"std\"] = settings[pretrained].get(\"std\")\n",
    "    return formatted_settings\n",
    "\n",
    "\n",
    "def get_preprocessing_fn(encoder_name, pretrained=\"imagenet\"):\n",
    "    params = get_preprocessing_params(encoder_name, pretrained=pretrained)\n",
    "    return functools.partial(preprocess_input, **params)\n",
    "\n",
    "class SegmentationModel(torch.nn.Module):\n",
    "\n",
    "    def initialize(self):\n",
    "        initialize_decoder(self.decoder)\n",
    "        initialize_head(self.segmentation_head)\n",
    "        if self.classification_head is not None:\n",
    "            initialize_head(self.classification_head)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\n",
    "        features = self.encoder(x)\n",
    "        decoder_output = self.decoder(*features)\n",
    "\n",
    "        masks = self.segmentation_head(decoder_output)\n",
    "\n",
    "        if self.classification_head is not None:\n",
    "            labels = self.classification_head(features[-1])\n",
    "            return masks, labels\n",
    "\n",
    "        return masks\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Inference method. Switch model to `eval` mode, call `.forward(x)` with `torch.no_grad()`\n",
    "        Args:\n",
    "            x: 4D torch tensor with shape (batch_size, channels, height, width)\n",
    "        Return:\n",
    "            prediction: 4D torch tensor with shape (batch_size, classes, height, width)\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            self.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.forward(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "class SegmentationHead(nn.Sequential):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, activation=None, upsampling=1):\n",
    "        conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        upsampling = nn.UpsamplingBilinear2d(scale_factor=upsampling) if upsampling > 1 else nn.Identity()\n",
    "        activation = Activation(activation)\n",
    "        super().__init__(conv2d, upsampling, activation)\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Sequential):\n",
    "\n",
    "    def __init__(self, in_channels, classes, pooling=\"avg\", dropout=0.2, activation=None):\n",
    "        if pooling not in (\"max\", \"avg\"):\n",
    "            raise ValueError(\"Pooling should be one of ('max', 'avg'), got {}.\".format(pooling))\n",
    "        pool = nn.AdaptiveAvgPool2d(1) if pooling == 'avg' else nn.AdaptiveMaxPool2d(1)\n",
    "        flatten = Flatten()\n",
    "        dropout = nn.Dropout(p=dropout, inplace=True) if dropout else nn.Identity()\n",
    "        linear = nn.Linear(in_channels, classes, bias=True)\n",
    "        activation = Activation(activation)\n",
    "        super().__init__(pool, flatten, dropout, linear, activation)\n",
    "        \n",
    "class Unet(SegmentationModel):\n",
    "    \"\"\"Unet_ is a fully convolution neural network for image semantic segmentation\n",
    "    Args:\n",
    "        encoder_name: name of classification model (without last dense layers) used as feature\n",
    "            extractor to build segmentation model.\n",
    "        encoder_depth (int): number of stages used in decoder, larger depth - more features are generated.\n",
    "            e.g. for depth=3 encoder will generate list of features with following spatial shapes\n",
    "            [(H,W), (H/2, W/2), (H/4, W/4), (H/8, W/8)], so in general the deepest feature tensor will have\n",
    "            spatial resolution (H/(2^depth), W/(2^depth)]\n",
    "        encoder_weights: one of ``None`` (random initialization), ``imagenet`` (pre-training on ImageNet).\n",
    "        decoder_channels: list of numbers of ``Conv2D`` layer filters in decoder blocks\n",
    "        decoder_use_batchnorm: if ``True``, ``BatchNormalisation`` layer between ``Conv2D`` and ``Activation`` layers\n",
    "            is used. If 'inplace' InplaceABN will be used, allows to decrease memory consumption.\n",
    "            One of [True, False, 'inplace']\n",
    "        decoder_attention_type: attention module used in decoder of the model\n",
    "            One of [``None``, ``scse``]\n",
    "        in_channels: number of input channels for model, default is 3.\n",
    "        classes: a number of classes for output (output shape - ``(batch, classes, h, w)``).\n",
    "        activation: activation function to apply after final convolution;\n",
    "            One of [``sigmoid``, ``softmax``, ``logsoftmax``, ``identity``, callable, None]\n",
    "        aux_params: if specified model will have additional classification auxiliary output\n",
    "            build on top of encoder, supported params:\n",
    "                - classes (int): number of classes\n",
    "                - pooling (str): one of 'max', 'avg'. Default is 'avg'.\n",
    "                - dropout (float): dropout factor in [0, 1)\n",
    "                - activation (str): activation function to apply \"sigmoid\"/\"softmax\" (could be None to return logits)\n",
    "    Returns:\n",
    "        ``torch.nn.Module``: **Unet**\n",
    "    .. _Unet:\n",
    "        https://arxiv.org/pdf/1505.04597\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_name: str = \"resnet34\",\n",
    "        encoder_depth: int = 5,\n",
    "        encoder_weights: str = \"imagenet\",\n",
    "        decoder_use_batchnorm: bool = True,\n",
    "        decoder_channels: List[int] = (256, 128, 64, 32, 16),\n",
    "        decoder_attention_type: Optional[str] = None,\n",
    "        in_channels: int = 3,\n",
    "        classes: int = 1,\n",
    "        activation: Optional[Union[str, callable]] = None,\n",
    "        aux_params: Optional[dict] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = get_encoder(\n",
    "            encoder_name,\n",
    "            in_channels=in_channels,\n",
    "            depth=encoder_depth,\n",
    "            weights=encoder_weights,\n",
    "        )\n",
    "\n",
    "        self.decoder = UnetDecoder(\n",
    "            encoder_channels=(3,32,24), #self.encoder.out_channels,\n",
    "            decoder_channels=decoder_channels,\n",
    "            n_blocks=encoder_depth,\n",
    "            use_batchnorm=decoder_use_batchnorm,\n",
    "            center=True if encoder_name.startswith(\"vgg\") else False,\n",
    "            attention_type=decoder_attention_type,\n",
    "        )\n",
    "\n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=decoder_channels[-1],\n",
    "            out_channels=classes,\n",
    "            activation=activation,\n",
    "            kernel_size=3,\n",
    "        )\n",
    "\n",
    "        if aux_params is not None:\n",
    "            self.classification_head = ClassificationHead(\n",
    "                in_channels=self.encoder.out_channels[-1], **aux_params\n",
    "            )\n",
    "        else:\n",
    "            self.classification_head = None\n",
    "\n",
    "        self.name = \"u-{}\".format(encoder_name)\n",
    "        self.initialize()\n",
    "        \n",
    "def initialize_decoder(module):\n",
    "    for m in module.modules():\n",
    "\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_uniform_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def initialize_head(module):\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "def swish(x, inplace: bool = False):\n",
    "    \"\"\"Swish - Described in: https://arxiv.org/abs/1710.05941\n",
    "    \"\"\"\n",
    "    return x.mul_(x.sigmoid()) if inplace else x.mul(x.sigmoid())\n",
    "\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        super(Swish, self).__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        return swish(x, self.inplace)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"torch: {torch.__version__}, catalyst: {catalyst.__version__}\")\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = CUDA_VISIBLE_DEVICES  # \"\" - CPU, \"0\" - 1 GPU, \"0,1\" - MultiGPU\n",
    "\n",
    "utils.set_global_seed(SEED)\n",
    "utils.prepare_cudnn(deterministic=True)\n",
    "monitoring_params = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name=\"efficientnet-b0\"\n",
    "depth = ENCODER_DEPTH\n",
    "Encoder = encoders[name][\"encoder\"]\n",
    "params = encoders[name][\"params\"]\n",
    "params.update(depth=depth)\n",
    "encoder = Encoder(**params)\n",
    "\n",
    "\n",
    "class enetv2(nn.Module):\n",
    "    def __init__(self, backbone, out_dim, out_channels):\n",
    "        super(enetv2, self).__init__()\n",
    "        self._out_channels = out_channels\n",
    "\n",
    "        inputs = NUMBER_OF_INPUT_LAYERS\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(inputs, inputs*2, 3, stride=1, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(inputs*2, inputs*4, 3, stride=1, padding=1, bias=False)\n",
    "        self.conv3 = nn.Conv2d(inputs*4, inputs*8, 3, stride=1, padding=1, bias=False)\n",
    "        self.conv4 = nn.Conv2d(inputs*8, inputs*16, 3, stride=1, padding=1, bias=False)\n",
    "        self.conv5 = nn.Conv2d(inputs*16, inputs*32, 3, stride=1, padding=1, bias=False)\n",
    "        self.mybn1 = nn.BatchNorm2d(inputs*2)\n",
    "        self.mybn2 = nn.BatchNorm2d(inputs*4)\n",
    "        self.mybn3 = nn.BatchNorm2d(inputs*8)\n",
    "        self.mybn4 = nn.BatchNorm2d(inputs*16)\n",
    "        self.mybn5 = nn.BatchNorm2d(inputs*32)\n",
    "\n",
    "        self.enet = encoder\n",
    "        encoder._blocks[11] = nn.Identity()\n",
    "        encoder._blocks[12] = nn.Sequential(\n",
    "                nn.Conv2d(112, 320, kernel_size=(1, 1), stride=(1, 1)),\n",
    "                nn.BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.ReLU6())\n",
    "        encoder._conv_head = nn.Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "\n",
    "    def extract(self, x):\n",
    "        x = F.relu6(self.mybn1(self.conv1(x)))\n",
    "        x = F.relu6(self.mybn2(self.conv2(x)))\n",
    "        x = F.relu6(self.mybn3(self.conv3(x)))\n",
    "        x = F.relu6(self.mybn4(self.conv4(x)))\n",
    "        x = F.relu6(self.mybn5(self.conv5(x)))\n",
    "        x = self.enet(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.extract(x)\n",
    "        return x\n",
    "    \n",
    "model = Unet(encoder_name=\"efficientnet-b0\", classes=CLASSES, encoder_depth=ENCODER_DEPTH, decoder_channels=DECODER_CHANNELS, in_channels=64, encoder_weights=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padit2(canvas):\n",
    "    \n",
    "    canvas2 = np.zeros((canvas.shape[0],32,32),dtype=np.float32)\n",
    "    canvas2[:,5:26,5:26] = canvas\n",
    "\n",
    "    canvas2[:,:5,5:26] = canvas[:,-5:,:]\n",
    "    canvas2[:,-6:,5:26] = canvas[:,:6,:]\n",
    "    canvas2[:,5:26,:5] = canvas[:,:,-5:]\n",
    "    canvas2[:,5:26,-6:] = canvas[:,:,:6]\n",
    "\n",
    "    canvas2[:,:5,:5] = canvas2[:,21:26,:5]\n",
    "    canvas2[:,:5,26:32] = canvas2[:,:5,5:11]\n",
    "    canvas2[:,26:32,:5] = canvas2[:,26:32,21:26]\n",
    "    canvas2[:,26:32,26:32] = canvas2[:,26:32,5:11]\n",
    "\n",
    "    return canvas2\n",
    "\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        images: List[Path],\n",
    "        masks: List[Path] = None,\n",
    "        transforms=None\n",
    "    ) -> None:\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        image_path = self.images[idx]        \n",
    "        image = np.load(image_path)\n",
    "\n",
    "        result = {\"image\": image}\n",
    "                \n",
    "        if self.masks is not None:\n",
    "            mask = np.load(self.masks[idx])[:5,:,:]\n",
    "            result[\"mask\"] = mask\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            result = self.transforms(**result)\n",
    "        \n",
    "        result[\"filename\"] = image_path.name\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomtoroidalcrop_single(mat, x, y):\n",
    "\n",
    "    return mat.take(range(mat.shape[0]), mode=\"wrap\", axis=0).take(range(x,x+32), mode=\"wrap\", axis=1).take(range(y,y+32), mode=\"wrap\", axis=2)\n",
    "    \n",
    "def randomtoroidalcrop(image, mask):\n",
    "\n",
    "    x,y=np.random.randint(0,21//2,2)\n",
    "\n",
    "    return randomtoroidalcrop_single(image, x, y), randomtoroidalcrop_single(mask, x, y)\n",
    "\n",
    "def np_to_pt(image):\n",
    "    \n",
    "    return torch.from_numpy(image)\n",
    "    \n",
    "def flip_1(img, mask):\n",
    "    img, mask = np.flip(img, axis=1), np.flip(mask, axis=1)\n",
    "    img[23:27] = img[[23,26,25,24]]\n",
    "    mask = mask[[0,1,4,3,2]] # swap W, E\n",
    "    return img, mask\n",
    "\n",
    "def flip_2(img, mask):\n",
    "    img, mask = np.flip(img, axis=2), np.flip(mask, axis=2)\n",
    "    img[23:27] = img[[25,24,23,26]]\n",
    "    mask = mask[[0,3,2,1,4]] # swap N, S\n",
    "    return img, mask\n",
    "\n",
    "def rotate90(img, mask):\n",
    "    img, mask = np.rot90(img, axes=(1,2)), np.rot90(mask, axes=(1,2))\n",
    "    img[23:27] = img[[26,23,24,25]]\n",
    "    mask = mask[[0,4,1,2,3]]\n",
    "    return img, mask \n",
    "\n",
    "def valid_transforms(image, mask):\n",
    "    image, mask = randomtoroidalcrop(image, mask)\n",
    "    image, mask = np_to_pt(image), np_to_pt(mask)\n",
    "    result = {\"image\": image}\n",
    "    result['mask'] = mask\n",
    "    return result\n",
    "    \n",
    "def train_transforms(image, mask):\n",
    "    if USE_DIHEDRAL_AUG: # need to do rotate/flip augment b4 toroi, because toroi expands matrix\n",
    "        if np.random.uniform() > 0.5:\n",
    "            image, mask = flip_1(image, mask)\n",
    "        if np.random.uniform() > 0.5:\n",
    "            image, mask = flip_2(image, mask)\n",
    "        if np.random.uniform() > 0.5:\n",
    "            image, mask = rotate90(image, mask)\n",
    "        pass\n",
    "\n",
    "    image, mask = randomtoroidalcrop(image, mask)\n",
    "    image, mask = np_to_pt(image), np_to_pt(mask)\n",
    "    \n",
    "    result = {\"image\": image}\n",
    "    result['mask'] = mask\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import RandomSampler\n",
    "def get_loaders(\n",
    "    images: List[Path],\n",
    "    masks: List[Path],\n",
    "    random_state: int,\n",
    "    valid_size: float = 0.2,\n",
    "    batch_size: int = 32,\n",
    "    num_workers: int = 4,\n",
    "    train_transforms_fn = None,\n",
    "    valid_transforms_fn = None,\n",
    ") -> dict:\n",
    "\n",
    "    indices = np.arange(len(images))\n",
    "\n",
    "    # Divide the data set into train and valid parts.\n",
    "    # For halite, divide dataset in 10. Take 1/10th of every 10th as valid.\n",
    "    full_length = len(images)\n",
    "    valid_indices = []\n",
    "    for i in range(0,full_length,(full_length//10)):\n",
    "        for a in range(i,i+(full_length//100)):\n",
    "            if a<full_length:\n",
    "                valid_indices.append(a)\n",
    "\n",
    "    train_indices = np.setdiff1d(range(full_length),valid_indices)\n",
    "\n",
    "    np_images = np.array(images)\n",
    "    np_masks = np.array(masks)\n",
    "    \n",
    "    # Creates our train dataset\n",
    "    train_dataset = SegmentationDataset(\n",
    "      images = np_images[train_indices].tolist(),\n",
    "      masks = np_masks[train_indices].tolist(),\n",
    "      transforms = train_transforms_fn,\n",
    "    )\n",
    "\n",
    "    # Creates our valid dataset\n",
    "    valid_dataset = SegmentationDataset(\n",
    "      images = np_images[valid_indices].tolist(),\n",
    "      masks = np_masks[valid_indices].tolist(),\n",
    "      transforms = valid_transforms_fn\n",
    "    )\n",
    "\n",
    "    # Arbitrary epoch length\n",
    "    sampler = RandomSampler(train_dataset, replacement=True, num_samples=BATCH_SIZE * min(len(images),EPOCH_LENGTH)//BATCH_SIZE)\n",
    "\n",
    "    # Catalyst uses normal torch.data.DataLoader\n",
    "    train_loader = DataLoader(\n",
    "      train_dataset,\n",
    "      batch_size=batch_size,\n",
    "      #shuffle=True, # no need to shuffle\n",
    "      num_workers=num_workers,\n",
    "      drop_last=True,\n",
    "      sampler=sampler\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "      valid_dataset,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=False,\n",
    "      num_workers=num_workers,\n",
    "      drop_last=True,\n",
    "    )\n",
    "\n",
    "    # OrderedDict of loaders\n",
    "    loaders = collections.OrderedDict()\n",
    "    loaders[\"train\"] = train_loader\n",
    "    loaders[\"valid\"] = valid_loader\n",
    "\n",
    "    return loaders\n",
    "\n",
    "ROOT = Path(TRAIN_DIR)\n",
    "\n",
    "train_image_path = ROOT / \"data/\"\n",
    "train_mask_path = ROOT / \"labels/\"\n",
    "\n",
    "ALL_IMAGES = sorted(train_image_path.glob(\"*.npy\"))\n",
    "ALL_MASKS = sorted(train_mask_path.glob(\"*.npy\"))\n",
    "\n",
    "np_images = np.array(ALL_IMAGES)\n",
    "np_masks = np.array(ALL_MASKS)\n",
    "\n",
    "batch_size = BATCH_SIZE \n",
    "\n",
    "loaders = get_loaders(\n",
    "    images=ALL_IMAGES,\n",
    "    masks=ALL_MASKS,\n",
    "    random_state=SEED,\n",
    "    train_transforms_fn=train_transforms,\n",
    "    valid_transforms_fn=valid_transforms,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LR\n",
    "encoder_learning_rate = ENCODER_LR\n",
    "\n",
    "layerwise_params = {\"encoder*\": dict(lr=encoder_learning_rate, weight_decay=0.00003)}\n",
    "\n",
    "model_params = utils.process_model_params(model, layerwise_params=layerwise_params)\n",
    "\n",
    "# Ranger seems to worker better than AdamW\n",
    "base_optimizer = RAdam(model_params, lr=learning_rate, weight_decay=0.0003)\n",
    "optimizer = Lookahead(base_optimizer)\n",
    "\n",
    "# Simple schedule seems to work as well as any other\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=SCHEDULER_FACTOR, patience=SCHEDULER_PATIENCE)\n",
    "\n",
    "num_epochs = NUM_EPOCHS\n",
    "logdir = LOG_DIR \n",
    "\n",
    "if is_fp16_used:\n",
    "    fp16_params = dict(opt_level=\"O1\") # params for FP16\n",
    "else:\n",
    "    fp16_params = None\n",
    "\n",
    "print(f\"FP16 params: {fp16_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from catalyst.contrib.nn import BCEDiceLoss\n",
    "\n",
    "criterion = BCEDiceLoss()\n",
    "\n",
    "callbacks = [\n",
    "    CriterionCallback(input_key=\"mask\"),\n",
    "    DiceCallback(input_key=\"mask\"),\n",
    "    CheckpointCallback(save_n_best=SAVE_N_BEST),\n",
    "    EarlyStoppingCallback(patience=EARLY_STOPPING_PATIENCE)\n",
    "] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.dl import SupervisedRunner\n",
    "runner = SupervisedRunner(device=device, input_key=\"image\", input_target_key=\"mask\")\n",
    "\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=loaders,\n",
    "    callbacks=callbacks,\n",
    "    logdir=logdir,\n",
    "    num_epochs=num_epochs,\n",
    "    fp16=fp16_params,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
